\documentclass[11pt]{article}

\usepackage{amsfonts}
%\usepackage{geometry}
\usepackage[paper=a4paper, 
            left=20.0mm, right=20.0mm, 
            top=25.0mm, bottom=25.0mm]{geometry}
\pagestyle{empty}
\usepackage{graphicx}
\usepackage{fancyhdr, lastpage, bbding, pmboxdraw}
\usepackage[usenames,dvipsnames]{color}
\definecolor{darkblue}{rgb}{0,0,.6}
\definecolor{darkred}{rgb}{.7,0,0}
\definecolor{darkgreen}{rgb}{0,.6,0}
\definecolor{red}{rgb}{.98,0,0}
\usepackage[colorlinks,pagebackref,pdfusetitle,urlcolor=darkblue,citecolor=darkblue,linkcolor=darkred,bookmarksnumbered,plainpages=false]{hyperref}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\pagestyle{fancyplain}
\fancyhf{}
\lhead{ \fancyplain{}{CS440: Introduction to AI} }
%\chead{ \fancyplain{}{} }
\rhead{ \fancyplain{}{\today} }
%\rfoot{\fancyplain{}{page \thepage\ of \pageref{LastPage}}}
\fancyfoot[RO, LE] {Page \thepage\ of \textcolor{black}{\pageref{LastPage}} }
\thispagestyle{plain}

%%%%%%%%%%%% LISTING %%%
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\usepackage{verbatim} % used to display code
\usepackage{fancyvrb}
\usepackage{acronym}
\usepackage{amsthm, amsmath}
\usepackage{tikz}
    \usetikzlibrary{calc, arrows, arrows.meta, positioning}
\usepackage{amssymb,amsmath,stackengine}
\stackMath
\usepackage{ifthen}
\usepackage{enumitem}


\VerbatimFootnotes % Required, otherwise verbatim does not work in footnotes!

\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{CadetBlue}{cmyk}{0.62,0.57,0.23,0}
\definecolor{lightlightgray}{gray}{0.93}

\lstset{
	%language=bash,                          % Code langugage
	basicstyle=\ttfamily,                   % Code font, Examples: \footnotesize, \ttfamily
	keywordstyle=\color{OliveGreen},        % Keywords font ('*' = uppercase)
	commentstyle=\color{gray},              % Comments font
	numbers=left,                           % Line nums position
	numberstyle=\tiny,                      % Line-numbers fonts
	stepnumber=1,                           % Step between two line-numbers
	numbersep=5pt,                          % How far are line-numbers from code
	backgroundcolor=\color{lightlightgray}, % Choose background color
	frame=none,                             % A frame around the code
	tabsize=2,                              % Default tab size
	captionpos=t,                           % Caption-position = bottom
	breaklines=true,                        % Automatic line breaking?
	breakatwhitespace=false,                % Automatic breaks only at whitespace?
	showspaces=false,                       % Dont make spaces visible
	showtabs=false,                         % Dont make tabls visible
	columns=flexible,                       % Column format
	morekeywords={__global__, __device__},  % CUDA specific keywords
}

\newcommand{\question}[1]{\section*{\normalsize #1}}
% \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
% \newcommand{\extraspace}[]{
%     \begin{center}
%         \textbf{Use this page for extra space.}
%     \end{center}
% }

% Custom envs %
\newenvironment{answercols}
	{\begin{center}\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
	\toprule
	\textbf{Assertion} & \textbf{Explanation} \\
	\midrule}
	{\\ \bottomrule\end{tabular}\end{center}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\vec}[1]{\textbf{#1}}

\newcommand{\squig}{{\scriptstyle\sim\mkern-3.9mu}}
\newcommand{\lsquigend}{{\scriptstyle\lhd\mkern-3mu}}
\newcommand{\rsquigend}{{\scriptstyle\rule{.1ex}{0ex}\rhd}}
\newcounter{sqindex}
\newcommand\squigs[1]{%
  \setcounter{sqindex}{0}%
  \whiledo {\value{sqindex}< #1}{\addtocounter{sqindex}{1}\squig}%
}
\newcommand\rsquigarrow[2]{%
  \mathbin{\stackon[2pt]{\squigs{#2}\rsquigend}{\scriptscriptstyle\text{#1\,}}}%
}
\newcommand\lsquigarrow[2]{%
  \mathbin{\stackon[2pt]{\lsquigend\squigs{#2}}{\scriptscriptstyle\text{\,#1}}}%
}


\begin{document}
\begin{center}
    {\Large \textsc{Written Assignment 4}}
\end{center}
\begin{center}
    Due: Friday 12/05/2025 @ 11:59pm EST
\end{center}

\section*{\textbf{Disclaimer}}
I encourage you to work together, I am a firm believer that we are at our best (and learn better) when we communicate with our peers. Perspective is incredibly important when it comes to solving problems, and sometimes it takes talking to other humans (or rubber ducks in the case of programmers) to gain a perspective we normally would not be able to achieve on our own. The only thing I ask is that you report who you work with: this is \textbf{not} to punish anyone, but instead will help me figure out what topics I need to spend extra time on/who to help. When you turn in your solution (please use some form of typesetting: do \textbf{NOT} turn in handwritten solutions), please note who you worked with.\newline










\question{Question 1: The Precariousness of RL (25 points)}
In the last written assignment you considered a loss function where each point was weighted with a nonzero coefficient $r^{(i)}$. You showed that the optimal solution was a function of the weights each sample was given. In this problem you will express an arbitrary supervised learning dataset (which we use in RL) into a dataset where samples are given weights:
\begin{enumerate}
    \item Whenever we have a dataset where each sample is unique (e.g. does not appear more than once), we are creating a dataset where each sample is given a uniform weight. Show that when a dataset contains duplicate points we are creating a dataset where samples are not given uniform weights.

    \item Now relax the weighting terms so that every weight $r^{(i)} \ge 0$ instead of $> 0$. Show that this loss function over a dataset of finite samples can be converted into a weighted sum over all possible data points.

    \item What happens to terms with weights of 0? Do they impact the solution at all? How do we know that the model will perform well on these points?
\end{enumerate}

\noindent\textbf{Note:} this is a \textbf{proof} question, meaning you must follow formal proof structure (see the examples of piazza for guidance). There is one exception: you do not have to follow formal proof structure when answering the last part, natural language is fine but I will not accept hand-wavy justification.
\newpage













\question{Question 2: Reward Function Flavors (25 points)}
In lecture, we talked about MDPs that are formulated with a reward function $R(s)$ (i.e. the reward only depends on the current state). However, sometimes MDPs are formulated with a reward function $R(s,a)$ (i.e. a reward function that depends on the action taken), or even $R(s,a,s')$ (i.e. a reward function that depends on the action taken and the way the action is resolved). In this problem, you will show that even though someone may choose one flavor of reward function over another, they are actually identical:
\begin{enumerate}
    \item Write the bellman equation that uses $R(s,a)$ and write the bellman equation that uses $R(s,a,s')$
    \item Show how an MDP with reward function $R(s,a,s')$ can be converted into a different MDP with reward $R(s,a)$ such that optimal policies in the new MDP correspond exactly to optimal policies in the original MDP.
    \item Show how an MDP with reward function $R(s,a)$ can be convered into a different MDP with reward $R(s)$ such that optimal policies in the new MDP correspond exactly to optimal policies in the original MDP.
\end{enumerate}

\noindent\textbf{Note:} this is a \textbf{proof} question, meaning you must follow formal proof structure (see the examples of piazza for guidance).
\newpage









\question{Question 3: Sum of Discounted Rewards vs. Max Reward (25 points)}
In lecture we defined the utility of a trajectory to be some additive combination of the rewards along that trajectory. So far this has taken two forms: additive rewards and discounted rewards. However, what happens if we define the utility of a trajectory as the maximum reward observed in that trajectory? Show that this utility function does not result in stationary preferences between trajectories (i.e. that such an agent may change its preference for the optimal trajectory as a function of time). Is it still possible to define autility function on trajectories such that a policy which maximizes the expected trajectory utility results in optimal behavior?\newline

\noindent\textbf{Note:} this is a \textbf{proof} question, meaning you must follow formal proof structure (see the examples of piazza for guidance).
\newpage










\question{Question 4: Proof that the Bellman Equation is a Contraction Function (25 points)}
In lecture we claimed that the bellman equation is a contraction function. Specifically, we said that, for any two vectors of utilities $\vec{u}$ and $\vec{u}'$:
$$||B(\vec{u}) - B(\vec{u}')||_{\infty} \le \gamma||\vec{u} - \vec{u}'||_{\infty}$$
\begin{enumerate}
    \item Show that, for any functions $f$ and $g$:
$$|\max\limits_{a} f(a) - \max\limits_{a} g(a)| \le \max\limits_{a} |f(a) - g(a)|$$
    \item Derive an expression for $\Big|\Big(B(\vec{u}) - B(\vec{u}')\Big)(s)\Big|$ and then apply the result from part 1 to complete the proof that the bellman equation is a contraction function.
\end{enumerate}

\noindent\textbf{Note:} this is a \textbf{proof} question, meaning you must follow formal proof structure (see the examples of piazza for guidance).
\newpage



\end{document}

